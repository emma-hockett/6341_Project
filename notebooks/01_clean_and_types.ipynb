{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-11T15:31:12.820676Z",
     "start_time": "2025-10-11T15:31:12.264440Z"
    }
   },
   "source": [
    "# Imports and configuration\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import src.utils.file_utils as fu\n",
    "import src.utils.schema_utils as su\n",
    "import src.helpers.clean_helpers as chelp\n",
    "\n",
    "cfg_clean = fu.load_config(\"clean\")\n",
    "cfg_schema = fu.load_config(\"schema\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:31:14.605796Z",
     "start_time": "2025-10-11T15:31:12.890997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read in raw data file\n",
    "raw_hmda_df = fu.load_parquet(\"hmda_raw\")\n",
    "print(raw_hmda_df.shape)"
   ],
   "id": "b1dbb2a25d755bbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /Users/c1burns/Documents/UTD/BUAN 6341/project_repo/data/interim/2024_combined_mlar_header.parquet\n",
      "(12236879, 85)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:31:14.633875Z",
     "start_time": "2025-10-11T15:31:14.623746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop columns that are not known prior to application except for target\n",
    "features_to_drop = su.get_columns_by_attribute(cfg_schema, \"role\", \"drop\")\n",
    "raw_hmda_df.drop(columns=features_to_drop, inplace=True)\n",
    "print(raw_hmda_df.shape)"
   ],
   "id": "2a9d9dc344055f58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12236879, 67)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:31:23.444021Z",
     "start_time": "2025-10-11T15:31:14.646161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Strip leading and trailing whitespace\n",
    "# All columns are currently string values, so we will perform this on all columns regardless of eventual datatype\n",
    "chelp.strip_string_columns_inplace(raw_hmda_df)"
   ],
   "id": "433f345546831b7b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:32:17.065922Z",
     "start_time": "2025-10-11T15:31:23.519176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for null-like values. Using a 1% sample to speed up the check\n",
    "NULL_LIKE = cfg_clean[\"clean\"][\"null_like\"]\n",
    "null_candidates = chelp.quick_null_like_check(raw_hmda_df, NULL_LIKE)\n",
    "null_candidates"
   ],
   "id": "798b87d2d3421f37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: null_like_fraction, dtype: object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:32:17.426668Z",
     "start_time": "2025-10-11T15:32:17.087512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for columns that contain only null values\n",
    "fully_null_cols = raw_hmda_df.columns[raw_hmda_df.isna().all()]\n",
    "print(\"Columns with only null values: \", fully_null_cols.tolist())"
   ],
   "id": "174a57a1b709849f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with only null values:  []\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:32:27.029979Z",
     "start_time": "2025-10-11T15:32:17.439968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Some columns have conflated both an \"Exempt\" flag and the feature value.  We need to identify these columns and separate the exempt flag into its own column\n",
    "\n",
    "# Find the features using exempt\n",
    "features = set(su.get_columns_by_attribute(cfg_schema, \"role\", \"feature\"))\n",
    "exempt  = set(su.get_columns_by_attribute(cfg_schema, \"exempt\", True))\n",
    "exempt_feature_cols = sorted(features & exempt) # We need the intersection so we exclude dropped columns\n",
    "print(\"Features using exempt: \", exempt_feature_cols)\n",
    "\n",
    "# Apply the transformation\n",
    "created_flags = chelp.apply_exempt_split(raw_hmda_df, exempt_feature_cols)\n",
    "print(\"Created: \", created_flags)"
   ],
   "id": "1a05f388db2dcc42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features using exempt:  ['combined_loan_to_value_ratio', 'county_code', 'debt_to_income_ratio', 'intro_rate_period', 'loan_term', 'multifamily_affordable_units', 'prepayment_penalty_term', 'property_value']\n",
      "Created:  ['combined_loan_to_value_ratio_exempt', 'county_code_exempt', 'debt_to_income_ratio_exempt', 'intro_rate_period_exempt', 'loan_term_exempt', 'multifamily_affordable_units_exempt', 'prepayment_penalty_term_exempt', 'property_value_exempt']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:35:09.192483Z",
     "start_time": "2025-10-11T15:32:27.045002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We have one exception case with income to clean up before conversion\n",
    "raw_hmda_df[\"income\"] = raw_hmda_df[\"income\"].replace(\"999999999\", pd.NA)\n",
    "\n",
    "# Convert columns to correct data types\n",
    "raw_hmda_df = chelp.convert_by_schema(raw_hmda_df, cfg_schema)\n",
    "print(raw_hmda_df.dtypes)"
   ],
   "id": "e8acd8e4869415a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_year                                    Int16\n",
      "lei                                    string[pyarrow]\n",
      "loan_type                                        Int16\n",
      "loan_purpose                                     Int16\n",
      "preapproval                                      Int16\n",
      "                                            ...       \n",
      "intro_rate_period_exempt                 bool[pyarrow]\n",
      "loan_term_exempt                         bool[pyarrow]\n",
      "multifamily_affordable_units_exempt      bool[pyarrow]\n",
      "prepayment_penalty_term_exempt           bool[pyarrow]\n",
      "property_value_exempt                    bool[pyarrow]\n",
      "Length: 75, dtype: object\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:35:09.266146Z",
     "start_time": "2025-10-11T15:35:09.226566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify which features are categorical\n",
    "categorical = set(su.get_columns_by_attribute(cfg_schema, \"type\", \"categorical\"))\n",
    "categorical_feature_cols = sorted(features & categorical) # We need the intersection so we exclude dropped columns\n",
    "print(\"Categorical features: \", categorical_feature_cols)"
   ],
   "id": "2465a68121ed7000",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features:  ['action_taken', 'applicant_age', 'applicant_age_above_62', 'applicant_credit_scoring_model', 'applicant_ethnicity_1', 'applicant_ethnicity_2', 'applicant_ethnicity_3', 'applicant_ethnicity_4', 'applicant_ethnicity_5', 'applicant_ethnicity_observed', 'applicant_race_1', 'applicant_race_2', 'applicant_race_3', 'applicant_race_4', 'applicant_race_5', 'applicant_race_observed', 'applicant_sex', 'applicant_sex_observed', 'balloon_payment', 'business_or_commercial_purpose', 'census_tract', 'co_applicant_age', 'co_applicant_age_above_62', 'co_applicant_credit_scoring_model', 'co_applicant_ethnicity_1', 'co_applicant_ethnicity_2', 'co_applicant_ethnicity_3', 'co_applicant_ethnicity_4', 'co_applicant_ethnicity_5', 'co_applicant_ethnicity_observed', 'co_applicant_race_1', 'co_applicant_race_2', 'co_applicant_race_3', 'co_applicant_race_4', 'co_applicant_race_5', 'co_applicant_race_observed', 'co_applicant_sex', 'co_applicant_sex_observed', 'combined_loan_to_value_ratio', 'construction_method', 'county_code', 'debt_to_income_ratio', 'initially_payable_to_institution', 'interest_only_payment', 'intro_rate_period', 'lien_status', 'loan_purpose', 'loan_term', 'loan_type', 'manufactured_home_land_property_interest', 'manufactured_home_secured_property_type', 'multifamily_affordable_units', 'negative_amortization', 'occupancy_type', 'open_end_line_of_credit', 'other_non_amortizing_features', 'preapproval', 'prepayment_penalty_term', 'reverse_mortgage', 'state_code', 'submission_of_application', 'total_units']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:35:15.015268Z",
     "start_time": "2025-10-11T15:35:09.303819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert columns to categorical\n",
    "chelp.to_pandas_categoricals(raw_hmda_df, categorical_feature_cols)"
   ],
   "id": "b4abbfaa0c43c4d0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:35:15.806103Z",
     "start_time": "2025-10-11T15:35:15.113273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking for illogical or extreme numerical values\n",
    "print(\"Negative/zero loan amounts: \", raw_hmda_df.query(\"loan_amount <= 0\").shape[0])\n",
    "print(\"Negative/zero property values: \", raw_hmda_df.query(\"property_value <= 0\").shape[0])\n",
    "print(\"Negative incomes: \", raw_hmda_df.query(\"income < 0\").shape[0])"
   ],
   "id": "38b1ecf38261f910",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative/zero loan amounts:  0\n",
      "Negative/zero property values:  0\n",
      "Negative incomes:  7757\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:35:15.967837Z",
     "start_time": "2025-10-11T15:35:15.825290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Negative incomes appear to be typos, so correcting values to positive\n",
    "mask = raw_hmda_df[\"income\"] < 0\n",
    "raw_hmda_df.loc[mask, \"income\"] = raw_hmda_df.loc[mask, \"income\"].abs()\n",
    "print(\"Negative incomes: \", raw_hmda_df.query(\"income < 0\").shape[0])"
   ],
   "id": "708f833e3e5adf0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative incomes:  0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:35:18.093102Z",
     "start_time": "2025-10-11T15:35:15.981772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Handling target variable action_taken\n",
    "# There are three categories of codes: approved, denied, and excluded.  The excluded categories are cases where the application was never completed, a borrower withdrew an application before a lending decision, etc.\n",
    "\n",
    "# Getting relevant configs\n",
    "action_cfg = cfg_clean[\"clean\"][\"action_taken\"]\n",
    "approved = set(action_cfg[\"approved\"])\n",
    "denied = set(action_cfg[\"denied\"])\n",
    "valid = approved | denied\n",
    "\n",
    "# Removes excluded observations and creates approved_flag target variable based on approved/denied\n",
    "raw_hmda_df = chelp.apply_action_taken_flag(raw_hmda_df, cfg_clean)\n",
    "\n",
    "# Sanity check result\n",
    "counts = raw_hmda_df[\"approved_flag\"].value_counts(dropna=False)\n",
    "print(\"Approved/Denied breakdown:\")\n",
    "print(counts)"
   ],
   "id": "f8234145bfe91c79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approved/Denied breakdown:\n",
      "approved_flag\n",
      "True     6693164\n",
      "False    2147948\n",
      "Name: count, dtype: int64[pyarrow]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:35:58.529731Z",
     "start_time": "2025-10-11T15:35:51.908088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save updated data frame to disk\n",
    "fu.save_parquet(raw_hmda_df, \"hmda_2024_typed\")"
   ],
   "id": "bd273a494bce53f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /Users/c1burns/Documents/UTD/BUAN 6341/project_repo/data/interim/hmda_2024_typed.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/c1burns/Documents/UTD/BUAN 6341/project_repo/data/interim/hmda_2024_typed.parquet')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T15:35:25.907945Z",
     "start_time": "2025-10-11T15:35:25.906361Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "68dec47f16ec527c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
